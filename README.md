# Burkina Data Collector & QA Assistant

Un outil complet pour :

- **Collecter** des pages web et des PDFs, les nettoyer, chunker et sauvegarder en JSONL.
- **Indexer** le corpus dans Qdrant avec des embeddings BGE-M3.
- **Interroger** le corpus via une API FastAPI (`/ask`) qui s'appuie sur Ollama pour la g√©n√©ration.
- **Visualiser et tester** l'assistant gr√¢ce √† une interface web moderne (chat + dict√©e + lecture audio + historique).

---

## 1. Architecture

```
frontend/ (HTML/CSS/JS)
app/main.py (API FastAPI + service d'indexation)
scripts/ (collecte, extraction, chunking, d√©duplication)
data/ (corpus, sorties JSONL, rapports)
Dockerfile + docker-compose.yml
```

- **FastAPI** expose `/ask`, `/health` et sert aussi le frontend (http://localhost:8000/).
- **Qdrant** stocke les vecteurs (collection `burkina_corpus`).
- **Ollama** h√©berge les mod√®les de g√©n√©ration (ex. `gemma3:1b`).
- **Frontend** (http://localhost:8000/frontend/index.html) : chat en fran√ßais, dict√©e vocale, √©coute des r√©ponses, historique stock√© dans `localStorage`.

![Vue d'ensemble de l'application](docs/images/landing.png)

![Chat avec dict√©e et audio](docs/images/chat.png)

---

## 2. Pr√©requis

| Composant | Version recommand√©e | Notes |
|-----------|---------------------|-------|
| Python | 3.12 | Pour ex√©cuter localement sans Docker |
| Docker / Docker Compose | Derni√®re version | Pour lancer l'ensemble en un clic |
| Ollama | ‚â• 0.4 | Installer sur l'h√¥te (macOS/Linux/WSL) |
| Mod√®le Ollama | `gemma3:1b` (par d√©faut) | `ollama pull gemma3:1b` |
| Mod√®le embeddings | `BAAI/bge-m3` | T√©l√©charg√© automatiquement via `SentenceTransformer` |

> ‚ö†Ô∏è Les scripts de collecte / chunking peuvent √©crire dans `data/`. Montez un volume si vous travaillez en Docker.

---

## 3. D√©marrage express (Docker)

1. **Installer / lancer Ollama** sur votre machine (hors compose) et charger le mod√®le :
   ```bash
   ollama serve                            # lance l'API http://localhost:11434
   ollama pull gemma3:1b                   # mod√®le par d√©faut utilis√© par l'API
   ```

2. **D√©marrer Qdrant + l'API + le frontend** :
   ```bash
   cd data_collector-main
   docker compose up --build
   ```

3. **Acc√©der aux services** :
   - Interface web : http://localhost:8000/frontend/index.html (ou directement http://localhost:8000/)
   - API `/ask` : http://localhost:8000/ask (POST JSON)
   - Qdrant (UI) : http://localhost:6333/

4. **Arr√™ter** :
   ```bash
   docker compose down
   ```

> üí° L'API appelle Ollama via `http://host.docker.internal:11434`. Sous Linux sans Docker Desktop, exposez manuellement le port (ou modifiez `docker-compose.yml`).

---

## 4. D√©marrage manuel (hors Docker)

```bash
# 1. Configuration Python
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt

# 2. Lancer Qdrant (Docker recommand√©)
docker run -p 6333:6333 -p 6334:6334 qdrant/qdrant:v1.9.6

# 3. Lancer Ollama sur l'h√¥te
ollama serve
ollama pull gemma3:1b

# 4. D√©marrer l'API FastAPI
PYTORCH_MPS_DISABLE=1 uvicorn app.main:app --host 0.0.0.0 --port 8000

# 5. Servir le frontend (optionnel si vous utilisez http://localhost:8000/)
python -m http.server 5500 --directory frontend
```

---

## 5. Pipeline de collecte & pr√©paration

Les scripts restent disponibles pour enrichir/synchroniser le corpus :

| √âtape | Commande | Description |
|-------|----------|-------------|
| Robots.txt | `python scripts/check_robots.py --url URL --out data/raw_html/robots.txt` | V√©rifie les r√®gles du site |
| Crawl | `python scripts/crawl_site.py --start-url URL --out-dir data/raw_html --max-pages 50` | Collecte de pages HTML |
| PDF ‚Üí JSONL | `python scripts/pdf_extract.py --pdf-dir data/raw_pdfs --out data/corpus.jsonl` | Extraction texte |
| Chunking | `python scripts/chunker.py --in data/corpus.jsonl --out data/chunks.jsonl --mode chars --max-chars 500 --overlap 50` | D√©coupe des textes |
| D√©dup | `python scripts/dedup.py --in data/chunks.jsonl --out data/dedup.jsonl` | Suppression de doublons |
| Nettoyage | `python scripts/clean_corpus.py --in data/dedup.jsonl --out data/corpus_cleaned.jsonl --min-words 50` | Filtrage |
| Indexation Qdrant | `python scripts/index_qdrant.py --input data/corpus_cleaned.jsonl --collection burkina_corpus --qdrant-url http://localhost:6333 --batch-size 64 --recreate --normalize` | G√©n√©ration d'embeddings + upsert |

> ‚ÑπÔ∏è Les scripts acceptent des options suppl√©mentaires (`--help`).

---

## 6. API `/ask`

- M√©thode : `POST http://localhost:8000/ask`
- Body JSON :
  ```json
  {
    "question": "Quelles sont les d√©marches pour obtenir un RCCM au Burkina Faso ?",
    "top_k": 4,
    "score_threshold": 0.4,
    "normalize": true,
    "timeout": 240,
    "ollama_model": "gemma3:1b"
  }
  ```
- R√©ponse :
  ```json
  {
    "answer": "...",
    "sources": [
      {"source": "mon_guide.pdf", "score": 0.73, "payload": {"url": "..."}}
    ]
  }
  ```
- Variables d'environnement utiles :
  | Nom | D√©faut | Description |
  |-----|--------|-------------|
  | `QDRANT_URL` | `http://qdrant:6333` | Endpoint Qdrant |
  | `QDRANT_COLLECTION` | `burkina_corpus` | Collection utilis√©e |
  | `EMBED_MODEL` | `BAAI/bge-m3` | Mod√®le SentenceTransformer |
  | `OLLAMA_URL` | `http://host.docker.internal:11434` | Endpoint Ollama |
  | `OLLAMA_MODEL` | `gemma3:1b` | Mod√®le g√©n√©ratif par d√©faut |
  | `ALLOWED_ORIGINS` | `http://localhost:8000,http://127.0.0.1:8000` | CORS pour le frontend |

---

## 7. Frontend

- Accessible via http://localhost:8000/frontend/index.html (ou `/` si mont√©).
- Fonctionnalit√©s :
  - Chat en fran√ßais, reformulation des r√©ponses (Markdown nettoy√©).
  - Dict√©e (Web Speech API `fr-FR`).
  - Lecture audio (SpeechSynthesis API, voix fran√ßaise si dispo).
  - Historique (localStorage), possibilit√© de ¬´ Reposer ¬ª ou ¬´ Afficher ¬ª une ancienne question.
- Pour servir s√©par√©ment : `python -m http.server 5500 --directory frontend`.

---

## 8. Licences & cr√©dits

Ce projet est sous licence **MIT** (voir `LICENSE`). Les principaux composants tiers sont √©galement sous licence permissive :

| Librairie / Service | Licence |
|---------------------|---------|
| FastAPI | MIT |
| Uvicorn | BSD |
| SentenceTransformers | Apache-2.0 |
| Transformers (HF) | Apache-2.0 |
| Qdrant Client | Apache-2.0 |
| Qdrant Server | Apache-2.0 |
| Ollama | MIT |
| BAAI/bge-m3 | MIT |
| Gemma / Phi / TinyLlama (chez Ollama) | MIT / Apache (selon mod√®le) |
| Bootstrap CSS-like design custom | MIT (personnalis√©) |

Veuillez vous r√©f√©rer aux d√©p√¥ts respectifs pour les d√©tails des licences de mod√®les.

---

## 9. Commandes utiles

| Action | Commande |
|--------|----------|
| Construire l'image | `docker compose build` |
| D√©marrer (API + Qdrant) | `docker compose up` |
| Arr√™ter | `docker compose down` |
| Indexer corpus (local) | `python scripts/index_qdrant.py ...` |
| Tester API | `curl -X POST http://localhost:8000/ask -H 'Content-Type: application/json' -d '{"question":"..."}'` |
| Acc√©der au frontend | `http://localhost:8000/frontend/index.html` |
| Acc√©der au dossier data dans le conteneur | `docker compose exec api ls data` |

---

## 10. Check-list avant la mise en service

- [ ] Qdrant est lanc√© (`docker compose up`).
- [ ] Ollama tourne sur l'h√¥te et le mod√®le souhait√© est t√©l√©charg√© (`ollama pull gemma3:1b`).
- [ ] Le corpus est index√© dans Qdrant (`scripts/index_qdrant.py`).
- [ ] Frontend accessible et fonctionnel (Tests vocaux + audio si n√©cessaire).

Bon hacking !
